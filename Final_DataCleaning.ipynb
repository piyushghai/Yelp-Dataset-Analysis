{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushghai/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import cPickle as pickle\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get the stopwords for preprocessing\n",
    "\n",
    "NLTK_STOPWORDS = set(stopwords.words('english'))\n",
    "MORE_STOPWORDS = set([line.strip() for line in open('more_stopwords.txt', 'r')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NLTK pipeline\n",
    "\n",
    "def lowercase(s):\n",
    "    return s.lower()\n",
    "\n",
    "def tokenize(s):\n",
    "    token_list = nltk.word_tokenize(s)\n",
    "    return token_list\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    return s.translate(None, string.punctuation)\n",
    "\n",
    "def remove_numbers(s):\n",
    "    return s.translate(None, string.digits)\n",
    " \n",
    "def remove_stopwords(token_list):\n",
    "    exclude_stopwords = lambda token : token not in NLTK_STOPWORDS\n",
    "    return filter(lambda tok : tok not in MORE_STOPWORDS, filter(exclude_stopwords, token_list))\n",
    "\n",
    "def stemming_token_list(token_list):\n",
    "    STEMMER = PorterStemmer()\n",
    "    return [STEMMER.stem(tok.decode('utf-8')) for tok in token_list]\n",
    "\n",
    "def restring_tokens(token_list):\n",
    "    return ' '.join(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean all the reviews by removing stop words as well as punctutation marks\n",
    "def process_reviews(data_set):\n",
    "    clean_data_set = []\n",
    "    for text in data_set:\n",
    "        text = lowercase(text)\n",
    "        text = remove_punctuation(text)\n",
    "        text = remove_numbers(text)\n",
    "\n",
    "        token_list = tokenize(text)\n",
    "        token_list = remove_stopwords(token_list)\n",
    "\n",
    "        token_list = stemming_token_list(token_list)\n",
    "        \n",
    "        try:\n",
    "            clean_data_set.append(restring_tokens(token_list))\n",
    "        except:\n",
    "            pass\n",
    "    return clean_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load the previously saved pickle file\n",
    "review_data = pickle.load( open( \"resto_review.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group all reviews per star rating and extract text out of them\n",
    "starsGroup = review_data.groupby('stars_review')\n",
    "\n",
    "text_star_1 = starsGroup.get_group(1.0)['text']\n",
    "text_star_2 = starsGroup.get_group(2.0)['text']\n",
    "text_star_3 = starsGroup.get_group(3.0)['text']\n",
    "text_star_4 = starsGroup.get_group(4.0)['text']\n",
    "text_star_5 = starsGroup.get_group(5.0)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional : To reduce the dataset size and prevent laptop from frying, reduce the dataset size by sampling\n",
    "#sampling = 5000 # No of rows to be sampled\n",
    "\n",
    "#text_star_1 = text_star_1.sample(sampling)\n",
    "#text_star_2 = text_star_2.sample(sampling)\n",
    "#text_star_3 = text_star_3.sample(sampling)\n",
    "#text_star_4 = text_star_4.sample(sampling)\n",
    "#text_star_5 = text_star_5.sample(sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add all the corresponding original labels to reviews\n",
    "label_star_1 = [1.0]*len(text_star_1)\n",
    "label_star_2 = [2.0]*len(text_star_2)\n",
    "label_star_3 = [3.0]*len(text_star_3)\n",
    "label_star_4 = [4.0]*len(text_star_4)\n",
    "label_star_5 = [5.0]*len(text_star_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Create test and training dataset. We use 80-20 sampling here. We can use 66-33 sampling too\n",
    "train_stars_1, test_stars_1, train_labels_stars_1, all_1stars_labels_test = train_test_split(text_star_1, label_star_1, test_size=0.30)\n",
    "train_stars_2, test_stars_2, train_labels_stars_2, all_2stars_labels_test = train_test_split(text_star_2, label_star_2, test_size=0.30)\n",
    "train_stars_3, test_stars_3, train_labels_stars_3, all_3stars_labels_test = train_test_split(text_star_3, label_star_3, test_size=0.30)\n",
    "train_stars_4, test_stars_4, train_labels_stars_4, all_4stars_labels_test = train_test_split(text_star_4, label_star_4, test_size=0.30)\n",
    "train_stars_5, test_stars_5, train_labels_stars_5, all_5stars_labels_test = train_test_split(text_star_5, label_star_5, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58739"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels_stars_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Cleaning all the reviews and building corpus out of them\n",
    "corpus_5stars = process_reviews(train_stars_5)\n",
    "corpus_4stars = process_reviews(train_stars_4)\n",
    "corpus_3stars = process_reviews(train_stars_3)\n",
    "corpus_2stars = process_reviews(train_stars_2)\n",
    "corpus_1stars = process_reviews(train_stars_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 5-star reviews after processing:  284902\n",
      "Number of 4-star reviews after processing:  174334\n",
      "Number of 3-star reviews after processing:  72107\n",
      "Number of 2-star reviews after processing:  47154\n",
      "Number of 1-star reviews after processing:  58739\n"
     ]
    }
   ],
   "source": [
    "print \"Number of 5-star reviews after processing: \", len(corpus_5stars)\n",
    "print \"Number of 4-star reviews after processing: \", len(corpus_4stars)\n",
    "print \"Number of 3-star reviews after processing: \", len(corpus_3stars)\n",
    "print \"Number of 2-star reviews after processing: \", len(corpus_2stars)\n",
    "print \"Number of 1-star reviews after processing: \", len(corpus_1stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating combined dataset for training, containing representation of all the 5 star ratings possible\n",
    "all_5_4_train = np.append(corpus_5stars, corpus_4stars)\n",
    "all_5_4_3_train = np.append(all_5_4_train, corpus_3stars)\n",
    "all_5_4_3_2_train = np.append(all_5_4_3_train, corpus_2stars)\n",
    "all_text_train = np.append(all_5_4_3_2_train, corpus_1stars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_text_train, open(\"all_text_train.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(corpus_5stars, open(\"corpus_5stars_train.p\", \"wb\"))\n",
    "pickle.dump(corpus_4stars, open(\"corpus_4stars_train.p\", \"wb\"))\n",
    "pickle.dump(corpus_3stars, open(\"corpus_3stars_train.p\", \"wb\"))\n",
    "pickle.dump(corpus_2stars, open(\"corpus_2stars_train.p\", \"wb\"))\n",
    "pickle.dump(corpus_1stars, open(\"corpus_1stars_train.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_5stars_test = process_reviews(test_stars_5)\n",
    "corpus_4stars_test = process_reviews(test_stars_4)\n",
    "corpus_3stars_test = process_reviews(test_stars_3)\n",
    "corpus_2stars_test = process_reviews(test_stars_2)\n",
    "corpus_1stars_test = process_reviews(test_stars_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_5_4_test = np.append(corpus_5stars_test, corpus_4stars_test)\n",
    "all_5_4_3_test = np.append(all_5_4_test, corpus_3stars_test)\n",
    "all_5_4_3_2_test = np.append(all_5_4_3_test, corpus_2stars_test)\n",
    "all_text_test = np.append(all_5_4_3_2_test, corpus_1stars_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_text_test, open(\"all_text_test.p\", \"wb\"))\n",
    "pickle.dump(corpus_5stars_test, open(\"corpus_5stars_test.p\", \"wb\"))\n",
    "pickle.dump(corpus_4stars_test, open(\"corpus_4stars_test.p\", \"wb\"))\n",
    "pickle.dump(corpus_3stars_test, open(\"corpus_3stars_test.p\", \"wb\"))\n",
    "pickle.dump(corpus_2stars_test, open(\"corpus_2stars_test.p\", \"wb\"))\n",
    "pickle.dump(corpus_1stars_test, open(\"corpus_1stars_test.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_text_train, open(\"all_text_train_final.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
