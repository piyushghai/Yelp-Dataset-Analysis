{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import cPickle as pickle\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get the stopwords for preprocessing\n",
    "\n",
    "NLTK_STOPWORDS = set(stopwords.words('english'))\n",
    "MORE_STOPWORDS = set([line.strip() for line in open('more_stopwords.txt', 'r')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NLTK pipeline\n",
    "\n",
    "def lowercase(s):\n",
    "    return s.lower()\n",
    "\n",
    "def tokenize(s):\n",
    "    token_list = nltk.word_tokenize(s)\n",
    "    return token_list\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    return s.translate(None, string.punctuation)\n",
    "\n",
    "def remove_numbers(s):\n",
    "    return s.translate(None, string.digits)\n",
    " \n",
    "def remove_stopwords(token_list):\n",
    "    exclude_stopwords = lambda token : token not in NLTK_STOPWORDS\n",
    "    return filter(lambda tok : tok not in MORE_STOPWORDS, filter(exclude_stopwords, token_list))\n",
    "\n",
    "def stemming_token_list(token_list):\n",
    "    STEMMER = PorterStemmer()\n",
    "    return [STEMMER.stem(tok.decode('utf-8')) for tok in token_list]\n",
    "\n",
    "def restring_tokens(token_list):\n",
    "    return ' '.join(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean all the reviews by removing stop words as well as punctutation marks\n",
    "def clean_reviews(data_set):\n",
    "    clean_data_set = []\n",
    "    for text in data_set:\n",
    "        text = lowercase(text)\n",
    "        text = remove_punctuation(text)\n",
    "        text = remove_numbers(text)\n",
    "\n",
    "        token_list = tokenize(text)\n",
    "        token_list = remove_stopwords(token_list)\n",
    "\n",
    "        token_list = stemming_token_list(token_list)\n",
    "        \n",
    "        try:\n",
    "            clean_data_set.append(restring_tokens(token_list))\n",
    "        except:\n",
    "            pass\n",
    "    return clean_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load the previously saved pickle file\n",
    "review_data = pickle.load( open( \"resto_review.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Sampling\n",
    "review_data = review_data.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group all reviews per star rating and extract text out of them\n",
    "starsGroup = review_data.groupby('stars_review')\n",
    "\n",
    "text_star_1 = starsGroup.get_group(1.0)['text']\n",
    "text_star_2 = starsGroup.get_group(2.0)['text']\n",
    "text_star_3 = starsGroup.get_group(3.0)['text']\n",
    "text_star_4 = starsGroup.get_group(4.0)['text']\n",
    "text_star_5 = starsGroup.get_group(5.0)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional : To reduce the dataset size and prevent laptop from frying, reduce the dataset size by sampling\n",
    "#sampling = 5000 # No of rows to be sampled\n",
    "\n",
    "#text_star_1 = text_star_1.sample(sampling)\n",
    "#text_star_2 = text_star_2.sample(sampling)\n",
    "#text_star_3 = text_star_3.sample(sampling)\n",
    "#text_star_4 = text_star_4.sample(sampling)\n",
    "#text_star_5 = text_star_5.sample(sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add all the corresponding original labels to reviews\n",
    "label_star_1 = [1.0]*len(text_star_1)\n",
    "label_star_2 = [2.0]*len(text_star_2)\n",
    "label_star_3 = [3.0]*len(text_star_3)\n",
    "label_star_4 = [4.0]*len(text_star_4)\n",
    "label_star_5 = [5.0]*len(text_star_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Create test and training dataset. We use 80-20 sampling here. We can use 66-33 sampling too\n",
    "train_stars_1, test_stars_1, train_labels_stars_1, test_labels_stars_1 = train_test_split(text_star_1, label_star_1, test_size=0.30)\n",
    "train_stars_2, test_stars_2, train_labels_stars_2, test_labels_stars_2 = train_test_split(text_star_2, label_star_2, test_size=0.30)\n",
    "train_stars_3, test_stars_3, train_labels_stars_3, test_labels_stars_3 = train_test_split(text_star_3, label_star_3, test_size=0.30)\n",
    "train_stars_4, test_stars_4, train_labels_stars_4, test_labels_stars_4 = train_test_split(text_star_4, label_star_4, test_size=0.30)\n",
    "train_stars_5, test_stars_5, train_labels_stars_5, test_labels_stars_5 = train_test_split(text_star_5, label_star_5, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_stars_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Cleaning all the reviews and building corpus out of them\n",
    "corpus_5stars_train = clean_reviews(train_stars_5)\n",
    "corpus_4stars_train = clean_reviews(train_stars_4)\n",
    "corpus_3stars_train = clean_reviews(train_stars_3)\n",
    "corpus_2stars_train = clean_reviews(train_stars_2)\n",
    "corpus_1stars_train = clean_reviews(train_stars_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 5-star reviews after processing:  1542\n",
      "Number of 4-star reviews after processing:  998\n",
      "Number of 3-star reviews after processing:  389\n",
      "Number of 2-star reviews after processing:  262\n",
      "Number of 1-star reviews after processing:  308\n"
     ]
    }
   ],
   "source": [
    "print \"Number of 5-star reviews after processing: \", len(corpus_5stars_train)\n",
    "print \"Number of 4-star reviews after processing: \", len(corpus_4stars_train)\n",
    "print \"Number of 3-star reviews after processing: \", len(corpus_3stars_train)\n",
    "print \"Number of 2-star reviews after processing: \", len(corpus_2stars_train)\n",
    "print \"Number of 1-star reviews after processing: \", len(corpus_1stars_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating combined dataset for training, containing representation of all the 5 star ratings possible\n",
    "all_5_4_train = np.append(corpus_5stars_train, corpus_4stars_train)\n",
    "all_5_4_3_train = np.append(all_5_4_train, corpus_3stars_train)\n",
    "all_5_4_3_2_train = np.append(all_5_4_3_train, corpus_2stars_train)\n",
    "all_text_train = np.append(all_5_4_3_2_train, corpus_1stars_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_stars_train = train_labels_stars_5 + train_labels_stars_4 + train_labels_stars_3 + train_labels_stars_2 + train_labels_stars_1\n",
    "all_stars_test = test_labels_stars_5 + test_labels_stars_4 + test_labels_stars_3 + test_labels_stars_2 + test_labels_stars_1\n",
    "pickle.dump(pd.DataFrame(all_stars_train)[0], open(\"all_stars_train.p\",\"wb\"))\n",
    "pickle.dump(pd.DataFrame(all_stars_test)[0], open(\"all_stars_test.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_text_train, open(\"all_text_train.p\", \"wb\"))\n",
    "pickle.dump(corpus_5stars_train, open(\"corpus_5stars_train.p\", \"wb\"))\n",
    "pickle.dump(corpus_4stars_train, open(\"corpus_4stars_train.p\", \"wb\"))\n",
    "pickle.dump(corpus_3stars_train, open(\"corpus_3stars_train.p\", \"wb\"))\n",
    "pickle.dump(corpus_2stars_train, open(\"corpus_2stars_train.p\", \"wb\"))\n",
    "pickle.dump(corpus_1stars_train, open(\"corpus_1stars_train.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_5stars_test = clean_reviews(test_stars_5)\n",
    "corpus_4stars_test = clean_reviews(test_stars_4)\n",
    "corpus_3stars_test = clean_reviews(test_stars_3)\n",
    "corpus_2stars_test = clean_reviews(test_stars_2)\n",
    "corpus_1stars_test = clean_reviews(test_stars_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_5_4_test = np.append(corpus_5stars_test, corpus_4stars_test)\n",
    "all_5_4_3_test = np.append(all_5_4_test, corpus_3stars_test)\n",
    "all_5_4_3_2_test = np.append(all_5_4_3_test, corpus_2stars_test)\n",
    "all_text_test = np.append(all_5_4_3_2_test, corpus_1stars_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_text_test, open(\"all_text_test.p\", \"wb\"))\n",
    "pickle.dump(corpus_5stars_test, open(\"corpus_5stars_test.p\", \"wb\"))\n",
    "pickle.dump(corpus_4stars_test, open(\"corpus_4stars_test.p\", \"wb\"))\n",
    "pickle.dump(corpus_3stars_test, open(\"corpus_3stars_test.p\", \"wb\"))\n",
    "pickle.dump(corpus_2stars_test, open(\"corpus_2stars_test.p\", \"wb\"))\n",
    "pickle.dump(corpus_1stars_test, open(\"corpus_1stars_test.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(starsGroup, open(\"starsGroup.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
